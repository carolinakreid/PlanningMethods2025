{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab21b4bf-008a-481a-acd9-8a6e78503a33",
   "metadata": {},
   "source": [
    "# Advanced Notebook - Working with PUMS Microdata\n",
    "\n",
    "CP201A - Fall 2025\n",
    "\n",
    "# Learning Objectives\n",
    "\n",
    "* Understand the PUMS dataset\n",
    "* Load and clean PUMS data from CSV files\n",
    "* Recategorize a categorical variable\n",
    "* Clean a numeric variable\n",
    "* Compute summary statistics from microdata\n",
    "* Handle weighted observations\n",
    "* Merge the housing units and persons tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eec796-8dd3-4c22-8ca5-7132762225c8",
   "metadata": {},
   "source": [
    "# What is PUMS?\n",
    "\n",
    "Per the [Census Bureau](https://www.census.gov/programs-surveys/acs/microdata.html):\n",
    "\n",
    ">The Census Bureauâ€™s American Community Survey (ACS) Public Use Microdata Sample (PUMS) files enable data users to create custom estimates and tables, free of charge, that are not available through ACS pretabulated data products.  The ACS PUMS files are a set of records from individual people or housing units, with disclosure protection enabled so that individuals or housing units cannot be identified.\n",
    "\n",
    "So, in short, while we've been working with pre-computed summary tables (each row summarizes data for a given geography) up until this point, now we will be working with the underlying microdata (each row represents one person or one housing unit).\n",
    "\n",
    "Much more info in the very helpful [ACS PUMS Handbook](https://www.census.gov/content/dam/Census/library/publications/2021/acs/acs_pums_handbook_2021.pdf).\n",
    "\n",
    "# Getting PUMS data\n",
    "\n",
    "There are three ways to access PUMS data:\n",
    "1. Download statewide housing units and persons CSV tables via File Transfer Protocol (FTP)\n",
    "2. Prepare a custom extract/summary table using the Census Bureau's Microdata Access Tool (MDAT)\n",
    "3. Pull specified variables from the Census Data API\n",
    "More info and links for all three options can be found [here](https://www.census.gov/programs-surveys/acs/microdata/access.html).\n",
    "\n",
    "The Microdata Access Tool is pretty cool and worth a look, but it's similar to data.census.gov as a whole in that it is probably too complicated for casual users and too oversimplified for power users. It *is* possible to pull PUMS data from the Census Data API, but it's not so easy to do so. The `census` package doesn't support this dataset (it works only for ACS and the decennial census), but you can still get the data by querying the relevant URL directly. I provide a quick example of how to do so at the end of this notebook, in case you want to do similar things in future; but **we're mostly going to work with PUMS data from CSV files today.** https://www.census.gov/programs-surveys/acs/microdata.html\n",
    "\n",
    "## Figuring out what variables and geographies we're interested in\n",
    "\n",
    "PUMS data can be a little overwhelming to work with at first, because there are so many variables and because we can't use the same geographies (counties, places, census tracts) that we use for tabulated ACS data. Instead, PUMS data are associated with Public Use Microdata Areas (PUMAs), which are contiguous areas composed of counties and census tracts with at least 100,000 population. \n",
    "\n",
    "The best data dictionary for PUMS variables is actually the `variables` page from the Census Data API: https://api.census.gov/data/2022/acs/acs5/pums/variables.html To check whether PUMS can help with your research question, you might start with this concise list of [topics covered in PUMS data](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/subjects_in_pums/2022_5yr_PUMS_Subjects.pdf); then I like to go to the Census Data API page and start searching for keywords. There are about 500 variables, so it can take some time to confirm that you've identified the most relevant variables for your question. Today we've made things simpler by providing an extract of certain variables.\n",
    "\n",
    "You can use the [TIGERweb](https://tigerweb.geo.census.gov/tigerweb/) interactive web map to check PUMA boundaries and identify which PUMAs you're interested in. But note that PUMA identifiers (e.g. `07510`) are not unique across states, so you need to combine state FIPS codes (`06` for California) and PUMA IDs (`07510`, part of San Francisco city and county) to uniquely identify a PUMA. In low-population areas, PUMAs often encompass multiple counties, but in denser regions like the Bay Area, populous counties are often represented by multiple PUMAs. PUMA IDs consist of five numbers, the first three of which correspond to the three-digit FIPS code of the PUMA's (primary) county. We can take advantage of this format to quickly select data from specific counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49acb70b-7548-42d4-8487-594ca090d7b3",
   "metadata": {},
   "source": [
    "## Reading in PUMS data from CSV files\n",
    "\n",
    "Reading in a CSV as a pandas DataFrame is easy: `df = pd.read_csv('filename.csv')`. This is often all that's needed, but if you're dealing with large tables, it is sometimes very useful to specify additional arguments to `read_csv` to speed things up and preserve data quality:\n",
    "* `nrows` lets us read in only that many rows. Something like `nrows=5` can be very helpful if you just want to take a quick look at which columns are in a given file.\n",
    "* `usecols` tells pandas which columns to read in. The full PUMS CSVs are huge and reading in all columns is time- and memory-intensive, so specifying only the columns we need can be a good idea. (This argument isn't actually necessary for today's CSV files, which I've pre-trimmed down to the variables we need, but you should be familiar with this option for when you start working with really big tables.)\n",
    "* `dtype` lets us tell pandas which datatypes to use for specific columns. We'll use this now to prevent pandas from misinterpreting the state and PUMA FIPS codes as numbers, when they're actually string-y IDs.\n",
    "\n",
    "There are many other optional arguments for `pd.read_csv()` and many other functions and methods we've been using. How can you learn about them? **Pro Jupyter notebook coding tip**: you can check the documentation for a given function or method by pressing Shift-Tab inside the parentheses. So you can type something like `pd.crosstab(` and then press Shift-Tab and Jupyter will show you lots of useful information about this function. Quicker and more convenient than googling the function's name. \n",
    "\n",
    "Looking up information like this, or using Google or ChatGPT to figure out how to do things, is *completely normal*, by the way, and you will find yourself doing so frequently for the rest of your programming life. I looked up many dozens of things while writing these labs.\n",
    "\n",
    "### Variables from the housing units table\n",
    "\n",
    "We have provided the following variables from the full PUMS housing units table. You can click a variable's name to see its data dictionary from the Census Data API.\n",
    "\n",
    "| Topic | Variable/Documentation | Label |\n",
    "|-|-|-|\n",
    "| Geography | [ST](https://api.census.gov/data/2022/acs/acs5/pums/variables/ST.json) | State code |\n",
    "| Geography | [PUMA10](https://api.census.gov/data/2022/acs/acs5/pums/variables/PUMA10.json) | PUMA code based on 2010 Census definition for data years 2012-2021 |\n",
    "| Geography | [PUMA20](https://api.census.gov/data/2022/acs/acs5/pums/variables/PUMA20.json) | PUMA code based on 2020 Census definition for data years 2022 and later |\n",
    "| Unique ID | [SERIALNO](https://api.census.gov/data/2022/acs/acs5/pums/variables/SERIALNO.json) | Housing unit/GQ person serial number |\n",
    "| Weighting | [WGTP](https://api.census.gov/data/2022/acs/acs5/pums/variables/WGTP.json) | Housing Unit Weight |\n",
    "| Building age | [YRBLT](https://api.census.gov/data/2022/acs/acs5/pums/variables/YRBLT.json) | When structure first built |\n",
    "| Building type | [BLD](https://api.census.gov/data/2022/acs/acs5/pums/variables/BLD.json) | Units in structure |\n",
    "| Housing tenure (rent/own) | [TEN](https://api.census.gov/data/2022/acs/acs5/pums/variables/TEN.json) | Tenure |\n",
    "| Housing cost | [GRNTP](https://api.census.gov/data/2022/acs/acs5/pums/variables/GRNTP.json) | Gross rent (monthly amount) |\n",
    "| Housing cost adjustment | [ADJHSG](https://api.census.gov/data/2022/acs/acs5/pums/variables/ADJHSG.json) | Adjustment factor for housing dollar amounts |\n",
    "| Household income | [HINCP](https://api.census.gov/data/2022/acs/acs5/pums/variables/HINCP.json) | Household income (past 12 months) |\n",
    "| Household income adjustment | [ADJINC](https://api.census.gov/data/2022/acs/acs5/pums/variables/ADJINC.json) | Adjustment factor for income and earnings dollar amounts |\n",
    "| Rent burden | [GRPIP](https://api.census.gov/data/2022/acs/acs5/pums/variables/GRPIP.json) | Gross rent as a percentage of household income past 12 months |\n",
    "\n",
    "I've provided a code snippet below that reads in the housing units table, using `usecols` and `dtype` to fine-tune the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ace0a2-d2df-4b0a-9f6f-8f084b2d6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs = pd.read_csv('psam_h06.csv')\n",
    "print(hhs.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df9438-9bf3-40ce-8588-e7038ab39fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hhs = pd.read_csv(\n",
    "    'psam_h06.csv',\n",
    "    usecols=['STATE',\n",
    "        'PUMA',\n",
    "        'SERIALNO',\n",
    "        'WGTP',\n",
    "        'YRBLT',\n",
    "        'BLD',\n",
    "        'TEN',\n",
    "        'GRNTP',\n",
    "        'ADJHSG',\n",
    "        'HINCP',\n",
    "        'ADJINC',\n",
    "        'GRPIP',\n",
    "    ],\n",
    "    dtype={'PUMA': str,\n",
    "    },\n",
    ")\n",
    "hhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e199ca-25d1-46c6-9b1a-9aa92da6ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(hhs['PUMA'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3984194-c980-4814-92e1-3fdb80504600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can select data for SF in CA\n",
    "hhs = hhs[(hhs['STATE'] == 6) & (hhs['PUMA'].str[:3].isin(['075']))]\n",
    "\n",
    "# Check how many rows we have remaining\n",
    "len(hhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb894c-b302-4377-a7b1-e4ff9474066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = pd.read_csv(\n",
    "    'psam_p06.csv',\n",
    "    usecols=[\n",
    "        'STATE',\n",
    "        'PUMA',\n",
    "        'SERIALNO',\n",
    "        'SPORDER',\n",
    "        'PWGTP',\n",
    "        'RAC1P',\n",
    "        'HISP',\n",
    "    ],\n",
    "    dtype={'PUMA': str,\n",
    "    },\n",
    ")\n",
    "\n",
    "persons = persons[(persons['STATE'] == 6) & (persons['PUMA'].str[:3].isin(['075']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25513d7-2d9e-470b-ae6e-c00e3d4ecf9d",
   "metadata": {},
   "source": [
    "# Recoding categorical variables\n",
    "\n",
    "`BLD` (units in structure) shows up looking like an integer. Does this field simply tell us the number of units in the building? Let's imagine for a moment that we are sloppy analysts who don't read the documentation first. (We've all been there.) One way we could try to check whether this is what `BLD` is truly telling us is to plot a histogram of `BLD` and see if it looks reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c879af-0693-4655-bff7-46b1ba00cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a quick and dirty histogram for a sanity check\n",
    "hhs['BLD'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08778d9-0c13-4d4f-979f-e2e6953bbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can use .value_counts() to see a tabular representation of the frequencies\n",
    "# dropna=False because we also want to inspect missingness\n",
    "hhs['BLD'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab82229-20aa-4d78-9b8b-c47934ebb7fb",
   "metadata": {},
   "source": [
    "Wait a minute. The number of units in structures maxes out below 10? The large majority of structures contain two units?? This looks very wrong.\n",
    "\n",
    "OK, now let's do what we should have done in the first place: take a look at the [API reference page](https://api.census.gov/data/2022/acs/acs5/pums/variables/BLD.json) for `BLD`. Aha! `BLD` is actually a *categorical* variable. Each \"integer\" value corresponds to a category. We can clean this up substantially, but first, let's look into those `NaN` values...\n",
    "\n",
    "## Handling missing values\n",
    "\n",
    "One difference between the data dictionary from the Census Data API and the CSV version of the PUMS data is that the data dictionary says `0` corresponds to \"N/A (GQ)\" (i.e., the \"household\" is in group quarters so the question doesn't apply), but we can see via `.value_counts()` above that there are no zeros, only `NaN` (\"not a number,\" i.e. missing data). We should think about how we want to handle missingness. We could drop records with missing data: [`df.dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html). Or we could fill missing data with an appropriate value: [`df.fillna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html). This latter approach is more appropriate here, so let's fill missing values in the `BLD` column with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a155fc8-8cf0-489c-b710-2c2b7387e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should first check whether all missing values occur in group-quarters rows.\n",
    "# We can do so with pd.crosstab(), a very handy tool both for quick diagnostics\n",
    "# and for more nuanced analyses\n",
    "pd.crosstab(\n",
    "    hhs['SERIALNO'].str.contains('GQ'),\n",
    "    hhs['BLD'].isna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f240b25-1d42-4d11-a49b-407cf8f3b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great, we've confirmed that only GQ rows have missing BLD values.\n",
    "# Now we can use .fillna() to fix this missingness! \n",
    "# As an added bonus, filling missing values allows us to convert BLD from floats to ints.\n",
    "# (Why was it float in the first place? Because NaN *is* a float:\n",
    "# https://stackoverflow.com/questions/48558973/why-is-nan-considered-as-a-float )\n",
    "hhs['BLD'] = hhs['BLD'].fillna(0).astype(int)\n",
    "hhs.head()\n",
    "\n",
    "# You could probably apply similar treatments to several other columns in our DataFrame..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d452c9c-c01a-418a-8e49-2b120c83026f",
   "metadata": {},
   "source": [
    "Let's *recode* `BLD` to group the default categories into groups that are meaningful for our interests/needs. This is easy to do in pandas, using the `.map()` method. You may recall this method from the extra materials from last week, where we created a dict that mapped census tract FIPS codes to neighborhood names. Here, we'll specify a mapping from integer values of `BLD` to meaningful categories.\n",
    "\n",
    "Let's group `BLD` into four categories:\n",
    "1. Single-family houses\n",
    "2. Small multifamily buildings (2-19 units)\n",
    "3. Large multifamily buildings (20 or more units)\n",
    "4. Other\n",
    "\n",
    "I've started you off with a dict containing the default meanings of each value of `BLD`. (I copy-pasted this from the BLD.json link above, then lightly cleaned it up by converting string keys to ints and sorting the keys.) Try editing `recode_BLD` to map the integer values into the four categories we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41beafd0-ac1f-49ee-ae02-04c633de71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_BLD = {\n",
    "    0: \"N/A (GQ)\",\n",
    "    1: \"Mobile Home or Trailer\",\n",
    "    2: \"One-family house detached\",\n",
    "    3: \"One-family house attached\",\n",
    "    4: \"2 Apartments\",\n",
    "    5: \"3-4 Apartments\",\n",
    "    6: \"5-9 Apartments\",\n",
    "    7: \"10-19 Apartments\",\n",
    "    8: \"20-49 Apartments\",\n",
    "    9: \"50 or More Apartments\",\n",
    "    10: \"Boat, RV, van, etc.\",\n",
    "}\n",
    "recode_BLD = {\n",
    "    0: \"Other\",\n",
    "    1: \"Other\",\n",
    "    2: \"Single-family house\",\n",
    "    3: \"Single-family house\",\n",
    "    4: \"Small multifamily building (2-20 units)\",\n",
    "    5: \"Small multifamily building (2-20 units)\",\n",
    "    6: \"Small multifamily building (2-20 units)\",\n",
    "    7: \"Small multifamily building (2-20 units)\",\n",
    "    8: \"Large multifamily building (20 or more units)\",\n",
    "    9: \"Large multifamily building (20 or more units)\",\n",
    "    10: \"Other\",\n",
    "}\n",
    "\n",
    "# Let's save the recoded variable into a new column.\n",
    "# .astype('category') takes advantage of pandas' Categorical data type:\n",
    "# https://pandas.pydata.org/docs/user_guide/categorical.html\n",
    "hhs['building_type'] = hhs['BLD'].map(recode_BLD).astype('category')\n",
    "hhs['building_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910e9fd-53ab-41a7-a42f-634c45331ed7",
   "metadata": {},
   "source": [
    "While we're at it, let's clean up `TEN` too. Check the [API data dictionary](https://api.census.gov/data/2022/acs/acs5/pums/variables/TEN.json), then try filling missingness appropriately and mapping ints to meaningful labels, in a new `tenure` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e983bf-f86c-42ba-a9b0-81bf38af3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_TEN = {\n",
    "    0: \"N/A (GQ/vacant)\",\n",
    "    1: \"Owned with mortgage or loan (include home equity loans)\",\n",
    "    2: \"Owned Free And Clear\",\n",
    "    3: \"Rented\",\n",
    "    4: \"Occupied without payment of rent\",\n",
    "}\n",
    "\n",
    "hhs['tenure'] = hhs['TEN'].fillna(0).map(recode_TEN).astype('category')\n",
    "hhs['tenure'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f9ba6-8dde-4315-af04-5f009f632310",
   "metadata": {},
   "source": [
    "## Combining two variables to form a new one\n",
    "\n",
    "Our `persons` table has a variable about race (`RAC1P`) and a variable about ethnicity (`HISP`). Let's create a new variable that expresses both race and ethnicity via five categories: NH White, NH Black, NH Asian, NH Other, and Hispanic.\n",
    "\n",
    "As usual, a good starting point is to check the data dictionaries for [`RAC1P`](https://api.census.gov/data/2022/acs/acs5/pums/variables/RAC1P.json) and [`HISP`](https://api.census.gov/data/2022/acs/acs5/pums/variables/HISP.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e720e6b-0d68-40b6-a060-cae5c9c46001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start by collapsing categories in RAC1P to just four\n",
    "persons['race'] = persons['RAC1P'].map({\n",
    "    1: 'White',\n",
    "    2: 'Black',\n",
    "    3: 'Other',\n",
    "    4: 'Other',\n",
    "    5: 'Other',\n",
    "    6: 'Asian',\n",
    "    7: 'Asian',  # or Other, arguably\n",
    "    8: 'Other',\n",
    "    9: 'Other',\n",
    "})\n",
    "\n",
    "# Then create race_ethnicity based on race\n",
    "persons['race_ethnicity'] = 'NH ' + persons['race']\n",
    "# And overwrite rows for Hispanic respondents\n",
    "persons.loc[persons['HISP'] != 1, 'race_ethnicity'] = 'Hispanic'\n",
    "# Convert this column to pd.Categorical\n",
    "persons['race_ethnicity'] = persons['race_ethnicity'].astype('category')\n",
    "\n",
    "persons['race_ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bf17a-4f24-4f3e-89bf-f3674500d5ab",
   "metadata": {},
   "source": [
    "# Cleaning numeric variables\n",
    "\n",
    "Let's take a look at some numeric variables in our dataset: `GRNTP`, meaning gross monthly rent, and `HINCP`, meaning household income over the past 12 months. We want to *clean* these variables: understand and potentially address any missingness, and understand and potentially address outlier values.\n",
    "\n",
    "## Check missingness\n",
    "\n",
    "Checking the [`GRNTP` data dictionary](https://api.census.gov/data/2022/acs/acs5/pums/variables/GRNTP.json), we see that a value of 0 means \"N/A (GQ/vacant/owned or being bought/occupied without rent payment)\". But one difference between the Census Data API version of PUMS data and the CSV version is that \"N/A\" values are often just empty (null) cells, so we should inspect our variables both for `0`s and `NaN`s (null values). \n",
    "\n",
    "We can quickly check the percentage of rows with missing values, as well as with 0s in the `GRNTP` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f9049-05e9-4743-9e68-b84729642a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of rows with missing values. .isna() identifies rows with missingness,\n",
    "# then .sum() converts True to 1 and False to 0 and adds up all the 1s.\n",
    "# Then divide by the full DF length to get the share that are null\n",
    "print(hhs['GRNTP'].isna().sum() / len(hhs))\n",
    "\n",
    "# Similar logic for calculating the percent of rows with 0 as the value\n",
    "print((hhs['GRNTP'] == 0).sum() / len(hhs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9806f-55d5-4e17-be1a-3a71b1720758",
   "metadata": {},
   "source": [
    "Hmm, OK, no zeros, but 62 percent of rows are missing values. Can we explain why this is the case? The data dictionary suggests that many \"N/A\"s are associated with housing units that are owned (rather than rented), vacant, or in group quarters. We can calculate a cross-tab with `tenure`, which we cleaned up earlier, to see whether this substantially explains the missingness in `GRNTP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72ab2c-58aa-40c8-a8ac-4d4414153b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"True\" refers to rows where GRNTP is missing (NaN)\n",
    "pd.crosstab(hhs['GRNTP'].isna(), hhs['tenure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db82402-fb9e-4a5f-b991-3b48883f0a00",
   "metadata": {},
   "source": [
    "Ah, very good. We can see that all the missingness in `GRNTP` (gross rent) occurs when `tenure` is not \"rented\". In this case, null values in `GRNTP` are appropriate, although you might consider setting gross rent to 0 for those few hundred lucky respondents occupying units without paying rent.\n",
    "\n",
    "## Check outliers\n",
    "\n",
    "Another important step in cleaning numeric data is identifying outliers and handling them appropriately.\n",
    "\n",
    "There is no one hard-and-fast rule about identifying outliers. Sometimes you may wish to retain the inner 98 percent of the distribution (omitting the top and bottom percent of the data); other times you may want to drop observations outside some multiple of the [interquartile range](https://online.stat.psu.edu/stat200/lesson/3/3.2) (this is sometimes called a \"[Tukey](https://en.wikipedia.org/wiki/John_Tukey) fence\"); sometimes certain values will be facially unreasonable, like negative values for headcounts or percentages greater than 100%. Ultimately, though, because the object of outlier detection is to identify bad or un-useful data, there is no substitute for careful consideration of relevant domain knowledge.\n",
    "\n",
    "Pandas' `.describe()` method is a good place to start, as it gives us a sense of the range and central tendency of the variables' distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9970e-bae6-4ef9-bd59-c4d08693ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs[['GRNTP', 'HINCP']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f567431-f10e-48df-9c8c-cec32b6e8a99",
   "metadata": {},
   "source": [
    "Note that the maximum value for `GRNTP` (monthly gross rent) is about 3 times the 75th percentile value, but the maximum value for `HINCP` (annual household income) is *ten times* the 75th percentile value. This already suggests that `HINCP` is likelier to have some outliers than `GRNTP`. \n",
    "\n",
    "Let's take a look at the distributions of `GRNTP` and `HINCP`, always a good idea when taking the temperature of a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056dccd6-3ec8-4e1a-bfb9-36bc983c5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['GRNTP'].plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6016b-79d3-47ea-a4e4-0c311a336858",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['HINCP'].plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd9dd7-e6d4-41c5-a31e-e29bef1d5263",
   "metadata": {},
   "source": [
    "That \"second hump\" above \\\\$6000 for `GRNTP` looks a little suspicious, but rents above \\\\$6000 are (sadly) not unheard of in the Bay Area. You might want to mark anything about \\\\$6000 as outliers, or not. (Stretch goal for later: can you use other PUMS variables, like [`BDSP`](https://api.census.gov/data/2022/acs/acs5/pums/variables/BDSP.json), to test whether these higher rents really are plausible?) Similarly, the `HINCP` distribution shows a few extremely high-earning households. We might cut things off at \\\\$1,000,000. Think about it and decide what you think is reasonable.\n",
    "\n",
    "Think also about how to handle outliers. There are three main ways:\n",
    "1. Bottom/top-coding or \"clipping\" outliers - overwriting values outside a specified range with the min/max of that range. You can use pandas' [`.clip()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.clip.html) method for this.\n",
    "2. Replacing outliers with nulls / `NaN` - this will exclude the rows in question from analyses pertaining to this variable, but will retain the rest of their data. You can set the relevant cells to `np.nan`.\n",
    "3. Dropping rows with outliers entirely - this will exclude the rows in question from *all* analyses. You already know how to select rows of a DataFrame based on numeric criteria.\n",
    "\n",
    "Once you've decided what to call outliers and how to handle them, try implementing your approach below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b3ed7-504f-4662-93cd-4c51b896f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will simply set HINCP > 1000000 to NaN\n",
    "import numpy as np\n",
    "hhs.loc[hhs['HINCP'] > 1000000, 'HINCP'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfc472-178b-4f28-83d1-db73dc9b7e21",
   "metadata": {},
   "source": [
    "## Adjusting dollar variables for inflation\n",
    "\n",
    "The 5-year PUMS data contain observations collected between 2018 and 2022. The real value of the dollar was fluctuating over this time period, so PUMS includes adjustment factors to convert amounts from different years into comparable real dollars. As noted in the [API reference for `HINCP`](https://api.census.gov/data/2022/acs/acs5/pums/variables/HINCP.json), we should use `ADJINC` to adjust the values in `HINCP`. Other variables (like `GRNTP`) use `ADJHSG` instead; check the documentation!\n",
    "\n",
    "`ADJINC` and `ADJHSG` are both integers that implicitly represent floating-point adjustment factors with six decimal points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f965611-4400-47f4-a7b3-a8754e695386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun fact: there is only one value of ADJINC for each year in the dataset\n",
    "print(hhs['ADJINC'].value_counts())\n",
    "\n",
    "# We need to convert these integers into floating-point numbers to use them\n",
    "hhs['ADJINC'] = hhs['ADJINC'] / 1000000\n",
    "\n",
    "# Now we can adjust HINCP\n",
    "hhs['HINCP'] = hhs['HINCP'] * hhs['ADJINC']\n",
    "\n",
    "hhs['HINCP'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddb617-01b7-4bdd-b104-6f55fd84c06f",
   "metadata": {},
   "source": [
    "## Binning numeric variables into categories/ranges\n",
    "\n",
    "We can convert quasi-continuous variables like `HINCP` into discrete, categorical variables using [`pd.cut()`](https://pandas.pydata.org/docs/reference/api/pandas.cut.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67beb6-243b-41b4-b38e-d3cd3af289c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['income_bin'] = pd.cut(\n",
    "    hhs['HINCP'],\n",
    "    [-99999, 0, 50000, 100000, 200000, 500000, 9999999],\n",
    "    labels=[\n",
    "        '$0 or negative',\n",
    "        '$1-$50,000',\n",
    "        '$50,001-$100,000',\n",
    "        '$100,001-$200,000',\n",
    "        '$200,001-$500,000',\n",
    "        'More than $500,000'\n",
    "    ]\n",
    ")\n",
    "\n",
    "hhs['income_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec32d7-ea3e-47b1-8f60-96bacc8a985b",
   "metadata": {},
   "source": [
    "# Computing summary statistics from microdata\n",
    "\n",
    "Now that we have cleaned our numeric variables, let's calculate summary statistics for them!\n",
    "\n",
    "The *mean* (average) is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71d3bf-fb73-4f44-a851-c0090e1f7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could compute the mean manually...\n",
    "hhs['HINCP'].sum() / hhs['HINCP'].count()\n",
    "\n",
    "# ...or just use the pandas built-in method\n",
    "hhs['HINCP'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d66e8-cfcc-44b1-8166-ce0821057868",
   "metadata": {},
   "source": [
    "Variance is a little more complex:\n",
    "\n",
    "$$ Var(X) = \\frac{1}{n - ddof} \\sum_{i=1}^n{(x_i - \\mu)^2},$$\n",
    "\n",
    "where $\\mu$ is the mean of $X$, and $ddof$ (delta degrees of freedom) is 0 for population variance or 1 for [sample variance](https://en.wikipedia.org/wiki/Bessel's_correction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa66c9e-4cda-4855-96ce-fc1b350508d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could compute variance manually...\n",
    "((hhs['HINCP'] - hhs['HINCP'].mean())**2).sum() / (hhs['HINCP'].count() - 1)\n",
    "\n",
    "# ...or just use the pandas built-in method\n",
    "hhs['HINCP'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24920535-0aca-44d4-9dd7-48fea925b0a2",
   "metadata": {},
   "source": [
    "Standard deviation is just the square root of variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8236f-65f1-4c10-a835-fb081741abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could compute standard deviation manually...\n",
    "hhs['HINCP'].var()**0.5\n",
    "\n",
    "# ...or just use the pandas built-in method\n",
    "hhs['HINCP'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810529f5-a1cf-4958-83da-68c0d05e9b01",
   "metadata": {},
   "source": [
    "The [standard error of the mean](https://en.wikipedia.org/wiki/Standard_error) is equal to the sample standard deviation $\\sigma_x$ divided by the square root of the sample size:\n",
    "\n",
    "$$\\sigma_\\bar{x} = \\frac{\\sigma_x}{\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d1e7c-e261-466f-8c2f-c497f7e8cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could compute the standard error of the mean manually...\n",
    "hhs['HINCP'].std() / hhs['HINCP'].count()**0.5\n",
    "\n",
    "# ...or just use the pandas built-in method\n",
    "hhs['HINCP'].sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0cfe54-0d6d-4bbf-8315-395560182994",
   "metadata": {},
   "source": [
    "We can derive margins of error for various confidence levels by multiplying the standard error by the appropriate z-value (1.645 for 90% confidence level, 1.960 for 95% confidence level, 2.576 for 99% confidence level):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85154648-f020-44e1-a92f-7c5d46d8c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% confidence level margin of error\n",
    "hhs['HINCP'].sem() * 1.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8e1ac-40ab-4907-8dfc-3bb2b82fee82",
   "metadata": {},
   "source": [
    "Finally, the median is also easy to calculate using a pandas built-in method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e472ce-9437-4f5d-ae7b-e1dafe9da44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['HINCP'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0700e-9356-4393-82cf-e2ab2fc81805",
   "metadata": {},
   "source": [
    "## Handling weighted observations\n",
    "\n",
    "But there's a catch! You may have noticed the `WGTP` and `PWGTP` variables in the housing units and persons tables, respectively. These variables represent the *weights* associated with each record. The Census Bureau estimates these weights such that the PUMS records, if expanded accordingly, would amount to the total population of the area. Both the housing unit and person weights vary substantially, so our sample mean is inaccurate unless we account for them.\n",
    "\n",
    "The formula for the [weighted mean](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean) is:\n",
    "\n",
    "$$\\bar{x} = \\frac{\\sum_{i=1}^n{w_i x_i}}{\\sum_{i=1}^n{w_i}},$$\n",
    "\n",
    "where $w$ are the weights and $x$ are the variable's values. We can use [numpy's `average` function](https://numpy.org/doc/stable/reference/generated/numpy.average.html) to quickly perform this calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c678b19-6e9d-4d5a-9794-e6ac291ab6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hardest part here is that we need to exclude rows where HINCP is NaN\n",
    "np.average(\n",
    "    hhs.loc[hhs['HINCP'].notna(), 'HINCP'],\n",
    "    weights=hhs.loc[hhs['HINCP'].notna(), 'WGTP']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad00f690-a61b-4e12-ac6f-8f018b58992e",
   "metadata": {},
   "source": [
    "The weighted average household income is about $8,000 less than the unweighted average. Using the weights makes a real difference!\n",
    "\n",
    "**Note:** *correctly* calculating standard errors for estimates based on PUMS microdata is technically complex and well beyond the scope of this lab. (If you're really interested, you can learn more in the technical documentation, specifically [PUMS Accuracy of the Data](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/accuracy/2018_2022AccuracyPUMS.pdf). It would be a nice stretch goal to use what you've learned in these labs to implement the Successive Difference Replication formula for calculating standard errors.) For today, we will stick with the *unweighted* variance, standard deviation, and standard error of the mean.\n",
    "\n",
    "See [here](https://stackoverflow.com/questions/20601872/numpy-or-scipy-to-calculate-weighted-median) if you need to compute a weighted *median*, and want to see some slick numpy methods in action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec21b0-4fd8-4b48-9195-7bfd5e63a0b4",
   "metadata": {},
   "source": [
    "# Merging the housing units and persons tables\n",
    "\n",
    "Sometimes we want to consider both housing unit-specific and person-specific variables in our analyses. This means we need to *merge* the `hhs` and `persons` DataFrames.\n",
    "\n",
    "Merging is a powerful, fundamental pandas operation, discussed in great detail in the pandas [user guide](https://pandas.pydata.org/docs/user_guide/merging.html), [tutorials](https://pandas.pydata.org/docs/getting_started/intro_tutorials/08_combine_dataframes.html), and [API reference](https://pandas.pydata.org/docs/reference/api/pandas.merge.html). We'll just see a quick example today, but this is definitely a topic worthy of further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba215d-39b0-4bac-b625-2b99c5d8b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basic syntax is left.merge(right, on=column or columns in common).\n",
    "# df.merge() defaults to an *inner join*, where only keys that appear in both\n",
    "# the left and right DataFrames will be retained. You can control this behavior\n",
    "# with the `how` keyword argument. Note that df.join() (which merges on indices)\n",
    "# defaults to a *left join*\n",
    "hhs.merge(persons, on='SERIALNO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1b0b4-9d96-4ad6-9d8f-8a3dfd10167b",
   "metadata": {},
   "source": [
    "We wind up with 122,950 rows, one for each row in `persons`. This is because every value of `SERIALNO` that appears in the `persons` table also appears in the `hhs` table. But *not* every `SERIALNO` that appears in `hhs` also appears in `persons`! Can you think why this is the case? (Hint: think about the various values of the `tenure` variable.)\n",
    "\n",
    "We have also wound up with some unattractive and unwieldy `_x` and `_y` columns. This happens when the left and right DataFrames contain columns with the same names. We can avoid this by merging only a subset of the columns in `persons` onto `hhs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3138e28-d384-4f1a-9aa1-3f1f900cfd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = hhs.merge(persons[['SERIALNO', 'SPORDER', 'PWGTP', 'race_ethnicity']], on='SERIALNO')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddc33c-7728-47a3-808e-b7bbfe93aa97",
   "metadata": {},
   "source": [
    "As a micro-capstone, let's use this merged table to show a cross-tabulation of some housing unit-related variables and some person-related variables. I'll provide an example; try to adapt it to examine some interactions you're curious about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a05783-1adc-4e3b-8231-07a9342db104",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(merged['income_bin'], merged['race_ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444401c-0747-4d01-9793-622bc423a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check (unweighted) median household income by race/ethnicity\n",
    "merged.groupby('race_ethnicity')['HINCP'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef28c8-9556-493b-a464-1f7e4aa37dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can calculate *weighted* mean household income by race/ethnicity of head of household,\n",
    "# using *lambda functions*, which are small anonymous functions that are really worth learning about\n",
    "\n",
    "# We select SPORDER == 1 to analyze only heads of households\n",
    "merged[(merged['SPORDER'] == 1) & merged['HINCP'].notna()].groupby('race_ethnicity').apply(lambda df: np.average(df['HINCP'], weights=df['WGTP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f4aee-8c65-40c4-b014-9be2916e6b12",
   "metadata": {},
   "source": [
    "# Saving your data (in a better format than CSV)\n",
    "\n",
    "Before we wrap up, let's save our data tables to disk. We could use `df.to_csv()` to write them to CSVs, but now is a good time to try out a different, [arguably superior](https://www.analyticsvidhya.com/blog/2022/06/stop-storing-data-in-csvs-vs-feather/) file format: `.feather` files. Feather files are usually smaller than CSVs, are dramatically faster to read and write (this is important as our tables get larger and larger), and can preserve datatypes like `pd.Categorical` and datetimes. They're also stable across multiple versions of pandas and intercompatible with R and other data science tools. Their only real disadvantage is that you can't open them in Notepad/Excel and manually inspect their contents.\n",
    "\n",
    "You'll need to `conda install pyarrow` (feather files are part of the [Apache Arrow](https://arrow.apache.org/) project) into your conda environment. Restart the kernel and rerun all cells in this notebook if necessary, then you can `pd.read_feather()` and `df.to_feather()` to your heart's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2748fc-f0d1-433b-9f6f-8d634db2642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.to_feather('hhs.feather')\n",
    "persons.to_feather('persons.feather')\n",
    "merged.to_feather('merged.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493d6e0-a1e6-4fea-b402-73558d822bb3",
   "metadata": {},
   "source": [
    "# Bonus example: Pulling PUMS data from the Census Data API\n",
    "\n",
    "The below cell shows how to use the `requests` library to pull PUMS data directly from the Census Data API. \n",
    "\n",
    "A few notes:\n",
    "* It's a good idea to always pull the variables `SERIALNO` (unique housing unit identifier) and `WGTP` (housing unit weight), as well as `SPORDER` (person number within household) and `PWGTP` (person weight) if you're pulling person-related variables; these let us uniquely identify each housing unit and person and weight their data appropriately.\n",
    "* You can pull housing unit-related *and* person-related variables in the same Census Data API request, as demonstrated by our pulling `BDSP` (number of bedrooms, a housing unit variable) and `SEX` (individual's sex, a person variable). The two tables come pre-merged.\n",
    "    * All persons in the same housing unit will share that housing unit's values for all housing unit-related variables, so if you're interested in analyzing housing unit-specific attributes, make sure to select only rows where `SPORDER == '01'` (i.e. heads of household) so you don't double-count housing units occupied by multiple persons. Or much better yet, make sure your data pull includes only housing unit-related variables so that the resulting table will include *all* housing units, even vacant ones (which would otherwise be implicitly discarded during the Census Data API's automatic [inner join](https://www.w3schools.com/sql/sql_join.asp) of housing units and persons).\n",
    "* Selecting geographies is tricky. You can't pass a comma-delimited list of PUMAs like you can do with census tracts; the finest geographic scale you can specify is states, like `for=state:06`. But you can use *predicates* (essentially selection criteria) to pull data for one PUMA at a time (`PUMA20=xxxxx`). You can't pull multiple PUMAs at once this way, so it may be faster to download all relevant data statewide and then select PUMAs afterwards.\n",
    "* You can also use predicates to pull only records matching certain criteria (e.g. `MAR=1` means \"return only married individuals\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618ad0c-3540-4b5b-806c-46661e8cd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# We'll structure our helper lists/dicts slightly differently than before.\n",
    "# First we'll write a list of variables for which to pull data\n",
    "variables_of_interest = [\n",
    "    'SERIALNO',  # unique housing unit identifier\n",
    "    'SPORDER',  # person number within household\n",
    "    'WGTP',\n",
    "    'PWGTP',\n",
    "    'BDSP',\n",
    "    'SEX',\n",
    "]\n",
    "# Then a list of *predicates*, which filter the data and also show up as columns\n",
    "predicates = [\n",
    "    'for=state:06',  # CA only\n",
    "    'PUMA20=07510',  # PUMA 07510 only\n",
    "    'MAR=1',  # married - https://api.census.gov/data/2022/acs/acs5/pums/variables/MAR.json\n",
    "]\n",
    "# And finally a dict to help us rename columns; we include only numeric\n",
    "# columns here so we can also use this to batch-convert columns to ints\n",
    "rename_dict = {\n",
    "    'WGTP': 'hh_weight',  # household weight\n",
    "    'PWGTP': 'person_weight',  # person weight\n",
    "    'BDSP': 'num_bedrooms',  # number of bedrooms (household attribute)\n",
    "    'SEX': 'sex',  # person's sex (1 = male; 2 = female),\n",
    "    'MAR': 'marital_status',  # 1 = married\n",
    "}\n",
    "\n",
    "# Now we prepare our API query URL using an f-string and the str.join() method\n",
    "api_key = '827a8aafba255606ced9b44cd4380ba61ad39003'\n",
    "query_url = f'https://api.census.gov/data/2022/acs/acs5/pums?get={','.join(variables_of_interest)}&key={api_key}&{'&'.join(predicates)}'\n",
    "print(query_url)\n",
    "\n",
    "# Send the API request and store the result in `r`\n",
    "r = requests.get(query_url)\n",
    "\n",
    "# Take a look at the first few items in the JSON representation of the resulting data\n",
    "print(r.json()[:5])\n",
    "\n",
    "# The 0th element of the JSON contains the column names, so separate that from the rest\n",
    "cols = r.json()[0]\n",
    "data = r.json()[1:]\n",
    "\n",
    "# And now create a DataFrame using `data` and `cols`\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# Convert some columns to ints\n",
    "for col in rename_dict.keys():\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# Rename some columns\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture (November 12, 2025): Cleaning and Recategorizing Survey Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Bringing a .csv file into Python\n",
    "\n",
    "Python can read in multiple forms of data, but the most common is a .csv file (\"comma separated values\").  We can easily import .csv data into Python.  The \"pd.\" tells Python to call up the panda function (this is like vocabulary - something to learn), to read the file as a csv, the name of the file, and that the delimiter is a comma. (A delimiter is what separates each of the columns, or array values, from one another.)\n",
    "\n",
    "(We are able to put the name of the CSV file alone as the parameter rather than the full file path since our file is in the same folder as our Python file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv('CP201Asurveydata.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a quick look at our data\n",
    "survey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can get information about our dataset by calling the \"info()\" function\n",
    "survey_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're going to rename two of our columns to be more coding friendly. And then limit our dataframe to those three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.rename(columns={'Average days spent in neighborhood per week': 'days_week',\n",
    "                        'Support increasing the supply of housing': 'housing_supply',\n",
    "                        'Rent or own': 'tenure'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_df = survey_df[[\"days_week\", \"housing_supply\", \"tenure\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Categorical \"nominal\" variables\n",
    "\n",
    "Let's start by looking at one of our nominal categorical variables: tenure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple way to look the distribution of a nominal variable is to request the value_counts().  \n",
    "#Note that I include the (dropna=False) option in order to be able to see if I have any missing values\n",
    "lecture_df[['tenure']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=lecture_df['tenure'], columns=\"Total\", dropna=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get the percents.  Here, we want to normalize (create percents) by the \"total\" value in the column\n",
    "pd.crosstab(index=lecture_df['tenure'], columns=\"Total\", normalize='columns', dropna=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Categorical \"ordinal\" variables\n",
    "\n",
    "We explore categorical ordinal variables--like age or our likert scale questions--in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_df[['housing_supply']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one cool thing that makes looking at ordinal data easier is to assign a category order\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Define a category type with the ordered flag set to True\n",
    "category_order = CategoricalDtype([\"Strongly Disagree\", \"Disagree\", \n",
    "    \"Neutral\", \"Agree\", \"Strongly Agree\", \"Don't Know/NA\"], ordered=True)\n",
    "\n",
    "lecture_df['housing_supply'] = lecture_df['housing_supply'].astype(category_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=lecture_df['housing_supply'], columns=\"Total\", dropna=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=lecture_df['housing_supply'], columns=\"Total\", normalize='columns', dropna=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It also makes it easier to look at the distribution visually\n",
    "lecture_df['housing_supply'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.xlabel('Need to Increase Housing Supply')\n",
    "plt.ylabel('Number of Responses')\n",
    "plt.title('Respondent Support for Increasing Housing Supply')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Recoding categorical variables\n",
    "\n",
    "This is where we truly don our magician hat!  We need to decide how we want to recode these for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(lecture_df['tenure'], lecture_df['housing_supply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(lecture_df['tenure'], lecture_df['housing_supply'], normalize=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Agree' and 'Disagree' groupings\n",
    "group_mapping = {\n",
    "    'Agree': 'Agree',\n",
    "    'Strongly Agree': 'Agree',\n",
    "    'Disagree': 'Disagree',\n",
    "    'Strongly Disagree': 'Disagree',\n",
    "    \"Don't Know/NA\" : 'Unknown',\n",
    "    \"Neutral\":'Neutral'\n",
    "}\n",
    "\n",
    "# Create a new column 'housing_group' for the grouped categories\n",
    "lecture_df['supply_group'] = lecture_df['housing_supply'].map(group_mapping)\n",
    "pd.crosstab(index=lecture_df['supply_group'], columns=\"Total\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(lecture_df['tenure'], lecture_df['supply_group'], normalize=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Dummy Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's turn our tenure variable into a dummy variable\n",
    "#Python can do it automatically, but I highly recommend intentionally coding your dummies.  I am going to create a dummy that \n",
    "#is equal to 1 for renters (renter_dv = 1) and 0 if it's an owner. What should I do with \"other\"?  \n",
    "\n",
    "#Recoding or aggregating variables can be done lots of ways.  When I started, I liked making very clear,\n",
    "#line by line codes.  For example, \n",
    "lecture_df['renter_dv']=lecture_df['tenure'].map({\"Rent\":1, \"Own\":0, \"Other\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we always need to check our coding - it's easy to make a mistake!\n",
    "lecture_df[['renter_dv', 'tenure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(lecture_df['renter_dv'], lecture_df['supply_group'], normalize=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting Likert Questions into a Numeric Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_df[\"supply_numeric\"] = survey_df[\"housing_supply\"].map({\n",
    "        \"Strongly Agree\": 5, \n",
    "        \"Agree\": 4, \n",
    "        \"Neutral\": 3,\n",
    "        \"Disagree\": 2, \n",
    "        \"Strongly Disagree\": 1, \n",
    "        \"Don't Know/NA\": np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_df[[\"housing_supply\", \"supply_numeric\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_df.groupby(\"renter_dv\")[\"supply_numeric\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Are these different?\n",
    "Can I say that renters (dv=1) are more likely to support new supply (4) than owners (3.55)? \n",
    "\n",
    "Just like with the ACS data, we have to address sampling error and the potential that these two values are not statistically significant from one another.  Because I'm now working with raw data, I can calculate the standard deviation, standard error, and MOE myself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group data and calculate mean, count, std\n",
    "group_stats = lecture_df.groupby(\"renter_dv\")[\"supply_numeric\"].agg([\"mean\", \"count\", \"std\"])\n",
    "print(group_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Standard error = std / sqrt(n)\n",
    "group_stats[\"se\"] = group_stats[\"std\"] / np.sqrt(group_stats[\"count\"])\n",
    "print(group_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create confidence intervals (90%)\n",
    "group_stats[\"ci_lower\"] = group_stats[\"mean\"] - 1.645 * group_stats[\"se\"]\n",
    "group_stats[\"ci_upper\"] = group_stats[\"mean\"] + 1.645 * group_stats[\"se\"]\n",
    "print(group_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (3.55 - 4.0) / np.sqrt(0.16**2 + 0.09**2)\n",
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_science] *",
   "language": "python",
   "name": "conda-env-data_science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

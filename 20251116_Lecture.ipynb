{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture November 17: Cleaning and Recategorizing Survey Data Part II\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Bringing a .csv file into Python\n",
    "\n",
    "We're going to start by bringing in a .csv file of ACS PUMS data (American Community Survey Public Use Microdata). https://www.census.gov/programs-surveys/acs/microdata/access.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums_df=pd.read_csv('pums.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also specify how many rows we want to look at\n",
    "pums_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning Numeric Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric variables refer to any variable that includes numbers, either integers (1, 4, 300) or floats (1.6, 4.56, 300.1543). When we work with raw numeric data, we want to explore their \"distribution\" - what is the mean and standard deviation?  What is the smallest value?  What is the largest value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to ask python to \"describe\" a variable for us\n",
    "pums_df['HINCP'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums_df[\"HINCP\"].hist(bins=30)\n",
    "plt.xlabel(\"Household Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Household Income in San Francisco\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums_df[\"HINCP\"].quantile([0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pums_df[\"HINCP\"].dropna()\n",
    "lower = x.quantile(0.01)\n",
    "upper = x.quantile(0.99)\n",
    "\n",
    "clean_df = pums_df[(pums_df[\"HINCP\"] >= lower) & (pums_df[\"HINCP\"] <= upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['HINCP'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_0_df = pums_df[pums_df[\"HINCP\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_0_df['HINCP'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok - now your turn - work in a group to figure out the mean income by renters versus owners.\n",
    "#You're going to have to use the codebook, recategorize Tenure, and then calculate means by group (hint: groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how about income by the age of the building where the householder lives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Cleaning Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('survey_text.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s start by taking a look at the contents of the \"biggest challenge\" column\n",
    "# Display unique values \n",
    "print(\"\\nUnique values:\")\n",
    "print(df['biggest_challenge'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns do you see?  Are people using different words for the same concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Coding\n",
    "In your group, decide on a few categories that you see in the data.  I'm going to work through an example using \"homelessness\" as a concept, distinct from housing.  I want to include a sentence in my report saying what share of people said homelessness is the biggest challenge by tenure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Standardize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize to lowercase\n",
    "df[\"lowercase_text\"] = df[\"biggest_challenge\"].str.lower()\n",
    "df[[\"lowercase_text\", \"biggest_challenge\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"homeless_dv\"] = df[\"lowercase_text\"].str.contains(\n",
    "    \"homeless|houseless|homelessness|homless\", na=False\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"homeless_dv\"] == 1, [\"homeless_dv\", \"lowercase_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Text Cleaning - Regex\n",
    "Another, more advanced approach is regex, which allows you to have more control \n",
    "\n",
    "Plain matching, what we did above, looks for exact substrings.  But with regex, we can provide more complex decision rules.  For example:\n",
    "\n",
    "- “find words starting with ‘hom’” → r\"\\bhom\"\n",
    "- “find rent or rental or renting” → r\"rent.*\"\n",
    "- “find respondents who mention crime or violence or police” → r\"crime|violence|police\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex version\n",
    "homeless_pattern = r\"afford\"\n",
    "\n",
    "df[\"afford_dv_regex\"] = df[\"lowercase_text\"].str.lower().str.contains(\n",
    "    homeless_pattern, \n",
    "    na=False\n",
    ").astype(int)\n",
    "\n",
    "df.loc[df[\"afford_dv_regex\"] == 1, [\"lowercase_text\", \"afford_dv_regex\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can you do a crosstab of your homeless_dv with tenure?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code a couple more with your group! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_science] *",
   "language": "python",
   "name": "conda-env-data_science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

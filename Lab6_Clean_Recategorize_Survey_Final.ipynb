{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CP201A Lab 6: Cleaning and Recategorizing Survey Data**\n",
    "\n",
    "#### November 12, 2025\n",
    "\n",
    "## Learning Objectives\n",
    "* Clean a dataset\n",
    "* Recategorize and analyze nominal, numeric, and Likert scale survey data\n",
    "* Use dummy variables (0 or 1) for regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Preparing for Data Analysis**\n",
    "\n",
    "### **1.1 Bringing a .csv file into Python**\n",
    "\n",
    "Python can read in multiple forms of data, but the most common is a .csv file (\"comma separated values\"). \n",
    "We can easily import .csv data into Python with `pd.read_csv(\"file_name.csv\")`\n",
    "\n",
    "The `pd.` tells Python to call up pandas and read a given file as a csv, which pandas will turn into a dataframe.\n",
    "\n",
    "*Note: We put the name of the CSV file alone as the parameter rather than the full file path since our file is in the same folder as our Python file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read our data in as a pandas dataframe\n",
    "survey_df = pd.read_csv('CP201Asurveydata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at our data\n",
    "survey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get information about our dataset by calling the \"info()\" function\n",
    "survey_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Codebook**\n",
    "\n",
    "Your codebook is a record of all the information about your dataset. For instance, this is where you'd keep track of each variable in your dataset and the specific survey question or parameter to which it refers.\n",
    "\n",
    "Decide how and where you want to develop your codebook. You can do it directly in your notebook as a markdown cell, or do it separately in Word or Excel.  But don't forget to keep a record of the changes you make!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename our columns to something more coding-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.rename(columns={'Survey Number': 'survey_num', \n",
    "                        'Day of the Week': 'day_of_week', \n",
    "                        'Time of Day': 'time',\n",
    "                        'Interviewer or self-administered': 'collection_mode',\n",
    "                        'Connection to neighborhood': 'connection_nbhd',\n",
    "                        'Average days spent in neighborhood per week': 'days_week',\n",
    "                        'Neighborhood recovered from pandemic': 'pandemic_recover',\n",
    "                        'Safe walking alone at night': 'safe_walking',\n",
    "                        'New housing has increased gentrification': 'gentrification',\n",
    "                        'Bikes lanes have increased traffic safety': 'bike_lane',\n",
    "                        'I feel connected to the people and community in this city.': 'connected',\n",
    "                        'Support increasing the supply of housing': 'housing_supply',\n",
    "                        'Support speed enforcement cameras': 'speed_camera',\n",
    "                        'Support sales tax to fund climate disaster preparedness': 'climate_tax',\n",
    "                        'Support investment in policing': 'policing', \n",
    "                        'Gender identity': \"gender_identity\",\n",
    "                        'Rent or own': 'tenure',\n",
    "                        'Biggest challenge facing city': 'biggest_challenge',\n",
    "                        'Notes (include data here on \"Other\" responses, observational notes.)': 'notes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Exploring and Cleaning Variables**\n",
    "\n",
    "Okay!  Now let's start exploring each of the variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.0 `NaN` values**\n",
    "\n",
    "Even though we didn't have an N/A or Don't Know option on our survey, some respondents chose not to answer certain questions. Let's take a look at these \"blank\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you must include the dropna=False option in order to be able to see if I have any missing values\n",
    "survey_df['tenure'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that there are special data types that deal with missing, undefined, and invalid values. `NaN` refers to \"Not a number\" and is a special floating-point value. \n",
    "\n",
    "When we read in our data from the csv, python automatically assigned all the blank cells with this special data type. Note that in this case, we are using `math.nan`, not `np.nan` which is another common type of `NaN` value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Nominal (categorical) variables**\n",
    "\n",
    "Let's start by looking at one of our simplest questions: Tenure.\n",
    "\n",
    "An easy way to look at the distribution of a nominal variable is to use `.value_counts()`.  We should always do this first to assess the distribution of variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df[['tenure']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df[['gentrification']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's `pd.crosstab()`'s time to shine! \n",
    "\n",
    "`pd.crosstab()` is a pandas function that makes a quick table showing how two (or more) variables relate. It basically counts how many times each combination (pair of values) occurs. \n",
    "\n",
    "For example, if I want to answer the following question: \"Which group believes new housing creates gentrification: people who rent or people who own?\" \n",
    "`.crosstab()` can help me do that by giving me the following combinations:\n",
    "* Rent and agree with new housing increased gentrification\n",
    "* Rent and don't agree with new housing increased gentrification\n",
    "* Own and gree with new housing increased gentrification\n",
    "* Own and don't agree with new housing increased gentrification\n",
    "\n",
    "I can also use `.crosstab()` on a single variable, and it will count how many times each value appears, like `value_counts()`. `normalize=True` turns those counts into percentages.\n",
    "\n",
    "Just like with ```value_counts()```, we need to specify that ```dropna = False``` in order to include our non-responses. This is important because when reporting on these results, we will need to indicate what share of folks didn't answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the percents for each tenure category\n",
    "pd.crosstab(index=survey_df['tenure'], columns=\"Total\", dropna=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalize=True to convert the table into percentages\n",
    "pd.crosstab(index=survey_df['tenure'], columns=\"Total\", normalize=True, dropna=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use crosstab to determine the frequency of responses across multiple variables\n",
    "pd.crosstab(survey_df['tenure'], survey_df['gentrification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's \"normalize\" the results, aka convert them to percentages\n",
    "pd.crosstab(survey_df['tenure'], survey_df['gentrification'], normalize=True)\n",
    "\n",
    "# Hmmm what do you notice about these percentages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we \"normalize\" data (aka create a relative measure by converting to percentages), it's important that we understand what the percentages are **out of**. In other words, what is our denominator?\n",
    "\n",
    "* If our percentages are taken within each column (i.e. the denominator is the sum of column values), then we are treating each column as a group.\n",
    "    * Therefore, when discussing our results we are **comparing values within each column**. \n",
    "* If our percentages are taken within each row (i.e. the denominator is the sum of row values), then we are treating each row as a group.\n",
    "    * Therefore, when discussing our results are are **comparing values within each row**.\n",
    "\n",
    "##### **QUESTION: Which comparison is more useful for the crosstabulation below?**\n",
    "\n",
    "<img src=\"Crosstab_normalize_example.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize across the \"index\" (aka the rows)\n",
    "pd.crosstab(survey_df['tenure'], survey_df['gentrification'], normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize across columns\n",
    "pd.crosstab(survey_df['tenure'], survey_df['gentrification'], normalize='columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.crosstab()` is one of the most powerful tools we have for exploring and understanding our data!\n",
    "\n",
    "But wait, our work doesn't stop at crosstabulations!\n",
    "\n",
    "Because our Likert scale includes different degrees of response (i.e., Agree *and* Strongly Agree), it's hard to quickly communicate interesting data just with the table above. Let's look at different ways to work with Likert data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Likert Variables**\n",
    "\n",
    "First, take a look at the data. It's always important to start by examining the data as is.\n",
    "\n",
    "Once we examine the data, we have several different options for how we can recode Likert scale data.\n",
    "1. **Group together Agree/Disagree Categories (to work with a binary)**\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Combine \"Agree\" with \"Strongly Agree\" and \"Disagree\" with \"Strongly disagree\"\n",
    "\n",
    "2. **Turn it into a numeric scale**\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Where a higher number means stronger agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2.1. Looking at the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df[['gentrification']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cool thing that will make graphing easier is to assign a **category order** for ordinal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Define a category type with the ordered flag set to True\n",
    "category_order = pd.CategoricalDtype([\"Strongly Disagree\", \"Disagree\", \n",
    "    \"Neutral\", \"Agree\", \"Strongly Agree\", \"Don't Know/NA\"], ordered=True)\n",
    "\n",
    "survey_df['gentrification_ordered'] = survey_df['gentrification'].astype(category_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['gentrification_ordered'].value_counts(sort=False, dropna=False) \n",
    "# Note, sorting is turned off because value_counts automatically sorts based on value, not category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great for viewing the data, but not so great for putting it into a regression... We'll get to that with our dummy variables:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2.2. Combine Agree and Disagree Categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our function-building skills to make a function that will turn:\n",
    "\n",
    "* \"Strongly Agree\" into \"Agree\"\n",
    "* \"Strongly Disagree\" into \"Disagree\"\n",
    "* And keep everything else the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likert_grouped(column_name):\n",
    "    '''\n",
    "    Simplifies agreement responses in a specified column of our survey results dataframe\n",
    "    '''\n",
    "    survey_df[f\"{column_name}_grouped\"] = survey_df[column_name].map({ # Creates a new column that copies the original specified column\n",
    "        # Within the new column, some values are replaced using the following rules\n",
    "        \"Strongly Agree\": \"Agree\", # Replace all \"Strongly Agree\" responses with \"Agree\"\n",
    "        \"Strongly Disagree\": \"Disagree\", # Replace all \"Strongly Disagree\" responses with \"Disagree\"\n",
    "\n",
    "        # We want to keep the rest of our values the same, so we have to specify their \"map\" value\n",
    "        \"Agree\": \"Agree\", \n",
    "        \"Disagree\": \"Disagree\",\n",
    "        \"Neutral\": \"Neutral\",\n",
    "        \"Don't Know/NA\": \"Don't Know/NA\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it out! See if it works for the gentrification question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the function\n",
    "likert_grouped(\"gentrification\")\n",
    "\n",
    "# Now let's check to see if it worked\n",
    "survey_df[[\"gentrification\",\"gentrification_grouped\"]].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2.3 Turn the Likert scale into a numeric scale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing to translate Likert responses to a 1-5 scale, from Strongly Disagree as a 1 to Strongly Agree as a 5.\\\n",
    "Let's make another function for that: then we can just use our function for whichever approach we prefer on whichever column we want! Cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likert_to_numeric(column_name):\n",
    "    '''\n",
    "    Converts Likert responses to numeric values in a specified column of our survey results dataframe\n",
    "    '''\n",
    "    # This function works the same as the last example, but the \"rules\" are different\n",
    "    survey_df[f\"{column_name}_numeric\"] = survey_df[column_name].map({\n",
    "        \"Strongly Agree\": 5, \n",
    "        \"Agree\": 4, \n",
    "        \"Neutral\": 3,\n",
    "        \"Disagree\": 2, \n",
    "        \"Strongly Disagree\": 1, \n",
    "        \"Don't Know/NA\": np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check! Let's make sure that worked, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the function\n",
    "likert_to_numeric(\"gentrification\")\n",
    "\n",
    "# Now let's check to see if it worked\n",
    "survey_df[[\"gentrification\",\"gentrification_grouped\",\"gentrification_numeric\"]].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **EXERCISE [5 mins]: Now pick a likert scale variable that you're interested in and clean it.**\n",
    "\n",
    "Consider how you want to deal with the Neutral and Don't Know/NA responses in particular. Would it be more useful to convert it into a numeric scale or just combine categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Numeric variables**\n",
    "\n",
    "Numeric variables refer to any variable that includes numbers, either integers (1, 4, 300) or floats (1.6, 4.56, 300.1543). When we work with raw numeric data, we want to explore their \"distribution\" - what is the mean and standard deviation?  What is the smallest value?  What is the largest value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as with the describe function above, we can ask to describe a single variable\n",
    "survey_df['days_week'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But our values are being read as an object rather than an integer or float. Why is that?\n",
    "survey_df['days_week'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It looks like there are some values that don't conform to the integer or float format (i.e. 0-7, 5-6, etc.)\n",
    "survey_df[['days_week']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... the plot thickens...\n",
    "\n",
    "Even though the \"days of the week\" question is numeric, the ranges provided (like \"4-5\") are read by pandas as a string, because of the dash. We can't calculate the average or the standard deviation right away...\n",
    "\n",
    "Here's what we'll do. In our numeric value column, there are four possibilities:\n",
    "1. The value is null (i.e., non-response)\n",
    "2. The value is a float or integer\n",
    "3. The value is a string because it's written as a range (with a – )\n",
    "4. The value is a number but is being read as a string anyways\n",
    "\n",
    "Let's write a function that runs through all of those possibilities and handles each value accordingly. Depending on what each value of our column is, pandas will do something different to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_range_to_top(x):\n",
    "    #1. The value is null: keep it that way\n",
    "    if pd.isna(x):\n",
    "        return np.nan \n",
    "    #2. The value is a float or integer: round it and keep it\n",
    "    if isinstance(x, (int, float)):\n",
    "        return round(x) \n",
    "    #3 and 4. The value is a string, for whatever reason\n",
    "    if isinstance(x, str):\n",
    "    #3. The value is a range: keep the upper value\n",
    "        if \"-\" in x: \n",
    "            parts = x.split('-')\n",
    "            return float(parts[-1])\n",
    "    #4. The value is a float being read as a string: convert it to a float and round up\n",
    "        return math.ceil(float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply this function to every row in the column using .apply()\n",
    "survey_df[\"days_week_clean\"] = survey_df[\"days_week\"].apply(clean_range_to_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check to see if it worked\n",
    "survey_df[\"days_week_clean\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, that's better!\n",
    "\n",
    "##### **QUESTION: What is a limitation to this function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Creating Dummy Variables**\n",
    "\n",
    "When we’re working with categorical data (like Rent vs. Own) or survey scales, we can’t jump straight into statistical tests. That’s where dummy variables come in — they turn our qualitative data into numbers (usually 0s and 1s) so we can use them in quantitative analysis.\n",
    "\n",
    "They also let us focus on one category at a time. For example, if we’re looking at TENURE, we can make a dummy variable just for “rent” — marking it as 1 and everything else as 0 — to see how “rent” compares to the rest of the responses.\n",
    "\n",
    "Here's a visual explanation of how we use dummy variables to turn categorical data into a binary matrix. We'll cover this more in the next lab, but converting these values to 1's and 0's is what allows us to do statistical testing and regression modelling with categorical data!\n",
    "\n",
    "<img src=\"dummy_variable.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dummy that is equal to 1 for renters and 0 for owners. \n",
    "\n",
    "# One way of dealing with \"Other\" is to convert it to np.nan    \n",
    "survey_df['rent_dv']=survey_df['tenure'].map({\n",
    "    \"Rent\":1, \n",
    "    \"Own\":0, \n",
    "    \"Other\":np.nan})\n",
    "\n",
    "survey_df['rent_dv'].value_counts(dropna=False)\n",
    "\n",
    "# This is fine for now, but you'll see later on that it makes it hard to run statistical tests if we have stray NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also set global mapping lists, so we can apply it to lots of variables that have the same coding.\n",
    "\n",
    "# Note that I could have skipped the Strongly Agree/Somewhat Agree (and Disagree) step above and just coded them into dummies here\n",
    "\n",
    "mapping_agree = {\n",
    "    'Strongly Agree':1,\n",
    "    'Agree': 1,\n",
    "    'Neutral': 0,\n",
    "    'Disagree': 0,\n",
    "    'Strongly Disagree':0,\n",
    "    \"Don't Know/NA\": 0, # Since this is an \"agree\" dummy variable we can convert everything that isn't \"agree\" to a 0\n",
    "    np.nan: 0 # We know there is a single nan value in the dataset, so let's code this as well\n",
    "}\n",
    "\n",
    "# Apply mapping to the 'housing' column\n",
    "survey_df['housing_supply_dv_agree'] = survey_df['housing_supply'].map(mapping_agree)\n",
    "survey_df['housing_supply_dv_agree'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can do the same thing for another variable. \n",
    "survey_df['gentrification_dv_agree'] = survey_df['gentrification'].map(mapping_agree)\n",
    "survey_df['gentrification_dv_agree'].value_counts(dropna=False)\n",
    "\n",
    "# And we can create a \"disagree\" map that we can then repeatedly use on variables as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **EXERCISE [5 mins]: Now convert a variable of your own choosing into a dummy variable.**\n",
    "\n",
    "Consider which categories you want to compare, and how the Neutral and Don't Know/NA responses fit into that comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, the goal is to repeat this process, perhaps using a function and/or for loop, to achieve something like this table for each variable that you're interested in examining. \n",
    "\n",
    "Note that you don't have to convert every single variable in the entire dataset into a dummy variable, just focus on what you're most interested in!\n",
    "\n",
    "<img src=\"housing_dummy_variable_example.png\" width=\"800\">\n",
    "\n",
    "**Woo hoo! Now you're a data cleaning expert!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
